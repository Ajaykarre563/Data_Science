{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFtbCpnQoME7",
        "outputId": "20b725d8-9c54-43f8-e6fb-bfaf532834b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "nZXBTs2CIdOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization - Tokenization is the process of splitting the text into smaller, meaningful units called tokens. These tokens can be words, subwords, phrases, or even characters, depending on the specific application.\n",
        "\n",
        "Types of Tokenization Techniques:-\n",
        "\n",
        "\n",
        "\n",
        "*   Sentence-level tokenization: Splitting text into sentences.\n",
        "*   Word-level tokenization: Splitting text into individual words.\n",
        "*   Word Punctuation Tokenization : Splitting text and punctuations into individual tokens.\n",
        "*   TreebankWord Tokenization: Splitting text and punctuations into individual tokens expect the last punctuation.\n",
        "*   Character tokenization : Splitting text into each character.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ax5AokVKok9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"I love my India.\n",
        "India is a Great Country!\n",
        "India's Finance Minister is Nirmala Ramam ji.\"\"\""
      ],
      "metadata": {
        "id": "o7mSJXYcoVE3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hLmS1cAowu9",
        "outputId": "2a2ac4b8-6c40-4f85-cd9e-b2d0e688c447"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love my India.\n",
            "India is a Great Country!\n",
            "India's Finance Minister is Nirmala Ramam ji.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Paragraph --> sentences\n",
        "from nltk.tokenize import sent_tokenize   #sentence tokenization\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3o9XILQoxsk",
        "outputId": "6cbe6521-f7ee-4d00-eaf2-d940279f63d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)  #returns sentences of a corpus in list format\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PWcs9ENpIse",
        "outputId": "b83ceb74-11ff-4819-9472-26cb378eb975"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I love my India.',\n",
              " 'India is a Great Country!',\n",
              " \"India's Finance Minister is Nirmala Ramam ji.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePYMmYoXpfx8",
        "outputId": "61404401-4e7d-45b0-83e1-ab79d1658605"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #paragraph --> words\n",
        " #sentence --> words\n",
        " from nltk.tokenize import word_tokenize  #word tokenization\n"
      ],
      "metadata": {
        "id": "BtofxOK2pM-q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(corpus)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLVrZGdEqRrP",
        "outputId": "f63b78aa-ff3b-4347-ba08-e46a27b773fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'love',\n",
              " 'my',\n",
              " 'India',\n",
              " '.',\n",
              " 'India',\n",
              " 'is',\n",
              " 'a',\n",
              " 'Great',\n",
              " 'Country',\n",
              " '!',\n",
              " 'India',\n",
              " \"'s\",\n",
              " 'Finance',\n",
              " 'Minister',\n",
              " 'is',\n",
              " 'Nirmala',\n",
              " 'Ramam',\n",
              " 'ji',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(words))\n",
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJP2rpA0qTkv",
        "outputId": "bf1daf56-89d3-4c0b-b1b2-ef614dbc4157"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize  #sentences into words with punctuation being a word\n"
      ],
      "metadata": {
        "id": "wmB_CEXAqkpl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_punt = wordpunct_tokenize(corpus)  #returns into words with punchtuation being a token\n",
        "words_punt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4HFek_eq3MG",
        "outputId": "11034f4d-7bc2-413a-bb75-f8c7c2610c93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'love',\n",
              " 'my',\n",
              " 'India',\n",
              " '.',\n",
              " 'India',\n",
              " 'is',\n",
              " 'a',\n",
              " 'Great',\n",
              " 'Country',\n",
              " '!',\n",
              " 'India',\n",
              " \"'\",\n",
              " 's',\n",
              " 'Finance',\n",
              " 'Minister',\n",
              " 'is',\n",
              " 'Nirmala',\n",
              " 'Ramam',\n",
              " 'ji',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "JRbasbGSq5dw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TbWT = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "fImTwOgTr0I7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TbWT.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMarogzIsFFg",
        "outputId": "79303ad5-91c2-49bb-a211-2c5f55b964f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'love',\n",
              " 'my',\n",
              " 'India.',\n",
              " 'India',\n",
              " 'is',\n",
              " 'a',\n",
              " 'Great',\n",
              " 'Country',\n",
              " '!',\n",
              " 'India',\n",
              " \"'s\",\n",
              " 'Finance',\n",
              " 'Minister',\n",
              " 'is',\n",
              " 'Nirmala',\n",
              " 'Ramam',\n",
              " 'ji',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#character tokenization\n",
        "\n",
        "corpus = str(corpus)\n",
        "\n",
        "corpus = list(corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1Ct2RHAuOne",
        "outputId": "dac97e76-6426-45f5-cc52-6044d7c6687b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', ' ', 'l', 'o', 'v', 'e', ' ', 'm', 'y', ' ', 'I', 'n', 'd', 'i', 'a', '.', '\\n', 'I', 'n', 'd', 'i', 'a', ' ', 'i', 's', ' ', 'a', ' ', 'G', 'r', 'e', 'a', 't', ' ', 'C', 'o', 'u', 'n', 't', 'r', 'y', '!', '\\n', 'I', 'n', 'd', 'i', 'a', \"'\", 's', ' ', 'F', 'i', 'n', 'a', 'n', 'c', 'e', ' ', 'M', 'i', 'n', 'i', 's', 't', 'e', 'r', ' ', 'i', 's', ' ', 'N', 'i', 'r', 'm', 'a', 'l', 'a', ' ', 'R', 'a', 'm', 'a', 'm', ' ', 'j', 'i', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"hello world\"\n",
        "print(list(s))  #character tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOsYsnU-uopr",
        "outputId": "3be401fe-f717-4ab1-ac62-bc0cf1ee5779"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "jZLA9N66IieQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming - Stemming is the process of reducing a word to its base form (or) root form by removing suffixes like ing,ed,es,s,etc.,\n",
        "The resulting \"stem\" may not be a valid word but represents the core meaning of related word variations.\n",
        "\n",
        "Example:\n",
        "*   Running - run, runner - run\n",
        "*   Eating - eat, eaten - eat, eat - eat\n",
        "*   going - go, gone - go, goes - go\n",
        "\n",
        "Types of Stemmers:\n",
        "\n",
        "\n",
        "1.   PorterStemmer : The most common stemming algorithm in NLP, developed by Martin Porter, applies a series of rules to remove suffixes.\n",
        "\n",
        "2.   RegexpStemmer :The RegexpStemmer is a stemming algorithm in Natural Language Processing (NLP) that uses regular expressions (regex) to define how words should be stemmed. It allows you to explicitly define your own rules based on patterns in the word, making it highly customizable.\n",
        "\n",
        "    > Backend Process of RegexpStemmer:\n",
        "\n",
        "      1.   Define a Regular Expression (Pattern): The user defines a regular expression to match a specific suffix or part of the word they want to stem/chop off. Regular expressions are highly versatile and allow matching characters, substrings, or specific word forms.\n",
        "\n",
        "      2. Apply Regular Expression to Words: The algorithm applies the regular expression to the words in the text. It matches the defined pattern and removes or replaces it based on the provided rule.\n",
        "\n",
        "      3. Return the Stemmed Word: Once the pattern is matched, the corresponding suffix or portion of the word is removed or modified, and the stemmed word is returned.\n",
        "       \n",
        "3. SnowballStemmer: SnowballStemmer, also known as Porter2 Stemmer, is an advanced and efficient stemming algorithm developed by Martin Porter, the creator of the original Porter Stemmer. SnowballStemmer provides a more flexible and linguistically rich approach to stemming, offering better precision while handling various language-specific rules.\n",
        "\n",
        "\n",
        "    SnowballStemmer supports multiple languages, including:\n",
        "\n",
        "    English\n",
        "    French\n",
        "    German\n",
        "    Spanish\n",
        "    Dutch\n",
        "    Italian\n",
        "    Russian\n",
        "    And others\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When to use Stemming:\n",
        "\n",
        "*   Stemming is useful in tasks where the focus is on word occurence like information retrival, text classification or search engines, where reducing words to a common form helps capture the underlying meaning across different world forms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cMQsifhDs4V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PorterStemmer"
      ],
      "metadata": {
        "id": "mtTCTIzVpGpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "x6KLl34R4CZe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assigining PorterStemmer class to a object\n",
        "porter_stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "nVBSrmVz4ElA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#executing porterstemming function\n",
        "porter_stemming.stem('running')  #we need to add stem to the object created to execute the stemming technique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dg5zOZCR4M3P",
        "outputId": "389834b3-43ee-47b8-f739-6398056c3e6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'run'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['eating','eaten','eats','eat','eatening','studying','studied','history','historical', 'programming','program','finally','finalized']"
      ],
      "metadata": {
        "id": "rRJwss1w4QT-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in words:\n",
        "  print(_,\"--->\",porter_stemming.stem(_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocJK8c0voh1q",
        "outputId": "e03f6650-d833-4bba-cb3e-791d65fae7d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eaten ---> eaten\n",
            "eats ---> eat\n",
            "eat ---> eat\n",
            "eatening ---> eaten\n",
            "studying ---> studi\n",
            "studied ---> studi\n",
            "history ---> histori\n",
            "historical ---> histor\n",
            "programming ---> program\n",
            "program ---> program\n",
            "finally ---> final\n",
            "finalized ---> final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegexpStemmer"
      ],
      "metadata": {
        "id": "RsUp0EnzpO6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "XRDZLgFIo59A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regexp_Stemmer = RegexpStemmer('ing$|ed$|es$|s$',min = 4) #specifying the removable suffixes in the Regexp paramter & minimum is the minimum No of letters to be returned aftr stemming."
      ],
      "metadata": {
        "id": "_AUqWeaFqY45"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regexp_Stemmer.stem('running')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nf0tG3TZqdEI",
        "outputId": "02d62c1a-32e6-4111-bcdb-258fdac37e93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'runn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['eating','eaten','eats','eat','eatening','studying','studied','history','historical', 'programming','program','finally','finalized']\n",
        "stems = [Regexp_Stemmer.stem(_) for _ in words]"
      ],
      "metadata": {
        "id": "gJxKjhgdq2Ya"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwPpXorPrD4b",
        "outputId": "1dc62a70-b96d-4d13-b323-dd8992d0cb25"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eat',\n",
              " 'eaten',\n",
              " 'eat',\n",
              " 'eat',\n",
              " 'eaten',\n",
              " 'study',\n",
              " 'studi',\n",
              " 'history',\n",
              " 'historical',\n",
              " 'programm',\n",
              " 'program',\n",
              " 'finally',\n",
              " 'finaliz']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word,stem in zip(words,stems):\n",
        "  print(f\"Original : {word} ---> Stemmed : {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c-q8cPraSR",
        "outputId": "cc596edf-d235-4e58-8d2d-d771dc27c915"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original : eating ---> Stemmed : eat\n",
            "Original : eaten ---> Stemmed : eaten\n",
            "Original : eats ---> Stemmed : eat\n",
            "Original : eat ---> Stemmed : eat\n",
            "Original : eatening ---> Stemmed : eaten\n",
            "Original : studying ---> Stemmed : study\n",
            "Original : studied ---> Stemmed : studi\n",
            "Original : history ---> Stemmed : history\n",
            "Original : historical ---> Stemmed : historical\n",
            "Original : programming ---> Stemmed : programm\n",
            "Original : program ---> Stemmed : program\n",
            "Original : finally ---> Stemmed : finally\n",
            "Original : finalized ---> Stemmed : finaliz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SnowballStemmer"
      ],
      "metadata": {
        "id": "K_Hu2oXisPPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "hTxw2OZ0rnDS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "1DJXFpJCtYia"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['eating','eaten','eats','eat','eatening','studying','studied','history','historical', 'programming','program','finally','finalized']\n",
        "\n",
        "stems = [snowball_stemmer.stem(_) for _ in words]"
      ],
      "metadata": {
        "id": "sISfinI-ta_a"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word,stem in zip(words,stems):\n",
        "  print(f\"Original : {word} ---> Stemmed : {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVOMtfHrtmpH",
        "outputId": "eb17675a-287d-4f14-8597-22acf90dd458"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original : eating ---> Stemmed : eat\n",
            "Original : eaten ---> Stemmed : eaten\n",
            "Original : eats ---> Stemmed : eat\n",
            "Original : eat ---> Stemmed : eat\n",
            "Original : eatening ---> Stemmed : eaten\n",
            "Original : studying ---> Stemmed : studi\n",
            "Original : studied ---> Stemmed : studi\n",
            "Original : history ---> Stemmed : histori\n",
            "Original : historical ---> Stemmed : histor\n",
            "Original : programming ---> Stemmed : program\n",
            "Original : program ---> Stemmed : program\n",
            "Original : finally ---> Stemmed : final\n",
            "Original : finalized ---> Stemmed : final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary on Stemming:"
      ],
      "metadata": {
        "id": "2Ha3kOPtuFt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PorterStemmer: Good for basic English stemming, but can sometimes over-stem  \n",
        "   or under-stem words.\n",
        "2. SnowballStemmer: More advanced, with support for multiple languages and\n",
        "   better accuracy, making it a preferred option for precision tasks.\n",
        "3. RegexpStemmer: Very flexible, allowing custom rules with regular  \n",
        "   expressions, but requires knowledge of regex to define effective patterns."
      ],
      "metadata": {
        "id": "j2gncWpTuKC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "N-XCMhnLuSkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a text normalization technique in Natural Language Processing (NLP) that reduces words to their base or root form, known as the lemma. Unlike stemming, which may chop off suffixes to produce a word that may not be valid, lemmatization considers the context and transforms the word into its meaningful base form.\n",
        "\n",
        "For example:\n",
        "\n",
        "Running becomes run (verb).\n",
        "Better becomes good (adjective).\n",
        "Mice becomes mouse (noun).\n",
        "\n",
        "Process of Lemmatization:\n",
        "\n",
        "  1. Part-of-Speech Tagging: Identifying the grammatical category of a word\n",
        "     (noun, verb, adjective, etc.) since the lemma of a word can change based on its part of speech.\n",
        "\n",
        "  2. Dictionary Lookup: Using a dictionary or vocabulary to find the base form\n",
        "     of the word.\n",
        "\n",
        "Lemmatization can be categorized based on the types of words it handles:\n",
        "\n",
        "1. Verb Lemmatization:Involves reducing different verb forms to their base form.\n",
        "Example:\n",
        "\"running\" → \"run\"\n",
        "\"was\" → \"be\"\n",
        "\n",
        "2. Noun Lemmatization:Focuses on converting plural nouns to their singular forms.\n",
        "Example:\n",
        "\"mice\" → \"mouse\"\n",
        "\"children\" → \"child\"\n",
        "\n",
        "\n",
        "3. Adjective Lemmatization:Involves converting adjectives to their base form.\n",
        "Example:\n",
        "\"better\" → \"good\"\n",
        "\"worst\" → \"bad\"\n",
        "\n",
        "4. Adverb Lemmatization:Reduces adverbs to their root form.\n",
        "Example:\n",
        "\"quickly\" → \"quick\"\n",
        "\"better\" → \"well\"\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "lNqMBtv0uWcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization Technique : WordNetLematizer"
      ],
      "metadata": {
        "id": "eZTytXF9uWVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNetLemmatizer is a lemmatization tool provided by the Natural Language Toolkit (NLTK) library in Python. It uses the WordNet lexical database to perform lemmatization, which is a process of reducing words to their base or root form (lemma) while considering their context, particularly the part of speech (POS).\n",
        "\n",
        "Backend Process of WordNetLemmatizer:\n",
        "1. Word Lookup:The lemmatizer starts by checking if the word exists in the WordNet database. WordNet organizes words into sets of synonyms (synsets) and provides various forms of each word, including its lemmas.\n",
        "\n",
        "2. Part-of-Speech Tagging:To accurately determine the lemma, WordNetLemmatizer requires the part of speech (POS) of the word. If the POS is not provided, it defaults to noun. The lemma of a word can change based on whether it is used as a noun, verb, adjective, or adverb.\n",
        "\n",
        "3. Finding the Lemma:After identifying the word and its POS, the lemmatizer retrieves the appropriate lemma from the WordNet database. It matches the word to its corresponding lemma based on the provided POS.\n",
        "\n",
        "4. Returning the Lemma:Finally, the lemmatizer returns the base form of the word. If the word does not exist in WordNet, it returns the original word."
      ],
      "metadata": {
        "id": "CIuCvNGLvguf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "RVRlf1dvtyka"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')  #downloading wordnet Database"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDEH5eX6v-Ju",
        "outputId": "52d4061c-a082-41d4-9471-b72758b52d1d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()  #WordNetLemmatizer(token,pos)"
      ],
      "metadata": {
        "id": "gQhPwDTvwFat"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['eating','eaten','eats','eat','eatening','studying','studied','history','historical', 'programming','program','finally','finalized']\n",
        "stems = [wordnet_lemmatizer.lemmatize(_) for _ in words]"
      ],
      "metadata": {
        "id": "v0QVZ3BDwQps"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word,stem in zip(words,stems):\n",
        "  print(word,\"--->\",stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWyfH-OCwX2O",
        "outputId": "79737643-a49c-4f15-fe3e-d1c449f40cab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eating\n",
            "eaten ---> eaten\n",
            "eats ---> eats\n",
            "eat ---> eat\n",
            "eatening ---> eatening\n",
            "studying ---> studying\n",
            "studied ---> studied\n",
            "history ---> history\n",
            "historical ---> historical\n",
            "programming ---> programming\n",
            "program ---> program\n",
            "finally ---> finally\n",
            "finalized ---> finalized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('sportly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nIKUOHEQwqLH",
        "outputId": "2cdefe44-71c0-467d-dd04-2b0eaaac2b9a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sportly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bl66mg3DwyVd",
        "outputId": "c611eb03-41fc-4bfc-b4cf-c7eab3dcb144"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratulation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('''NOUN - n,\n",
        "VERB - v,\n",
        "ADJECTIVE - a,\n",
        "ADVERB - r''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW9_T1lFw3OZ",
        "outputId": "8637e152-cd3f-498d-8aa6-cd73546fe4a0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOUN - n,\n",
            "VERB - v,\n",
            "ADJECTIVE - a,\n",
            "ADVERB - r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('running') #defaulty the pos is Noun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FOCpUbncxNet",
        "outputId": "b45edd5d-ce4b-4d50-df5d-379f856a0fc3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'running'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('running',pos = 'v')  #now we are considering the verb as part of speech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S7w6Zb-uxb26",
        "outputId": "0abefb7e-9ed8-4b6f-81fb-3f1799acffd8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'run'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('was',pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uftD4QpRxfTH",
        "outputId": "fe812b66-7974-4193-a3e7-991b2c9ab21e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('better',pos = 'a')  #adjective as POS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h7iufrq4xuG0",
        "outputId": "35dfed28-d08b-41fb-e7e4-f5ea3976d6eb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('mice',pos = 'n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LM2UkO1FxwUM",
        "outputId": "8d626069-4975-476c-fea8-23bd46612d84"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mouse'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "1YlDwOOpyj8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation.We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.\"\"\""
      ],
      "metadata": {
        "id": "lpohN2Gbx2QX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Kio5061zTG_Q",
        "outputId": "2eb46add-4d61-4634-fbab-bd5b5525278f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation.We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#performing tokenizing\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gthLa8-aYi9d",
        "outputId": "8659a0c4-9348-4209-b99b-5bf57b2fdf40"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(corpus)\n",
        "\n",
        "\n",
        "\n",
        "print(type(sentences))\n",
        "print(len(sentences))\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sKUczEJY0DD",
        "outputId": "f1eefebd-959f-4767-a210-e2fb42613292"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture and their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.',\n",
              " 'That is why my FIRST VISION is that of FREEDOM.',\n",
              " 'I believe that India got its first vision of this in 1857, when we started the war of Independence.',\n",
              " 'It is this freedom that we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.We have 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'MY SECOND VISION for India is DEVELOPMENT.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among top five nations in the world in terms of GDP.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#performing word tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "Pu0KTDIqZQsu"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "  words_in_sentence = word_tokenize(sentence)\n",
        "  words.append(words_in_sentence)\n",
        "\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9pKJF8ZZWXA",
        "outputId": "9ddddb1a-d5de-49ff-d04d-8cbfa3a945a1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'have', 'three', 'visions', 'for', 'India', '.'], ['In', '3000', 'years', 'of', 'our', 'history', 'people', 'from', 'all', 'over', 'the', 'world', 'have', 'come', 'and', 'invaded', 'us', ',', 'captured', 'our', 'lands', ',', 'conquered', 'our', 'minds', '.'], ['From', 'Alexander', 'onwards', 'the', 'Greeks', ',', 'the', 'Turks', ',', 'the', 'Moguls', ',', 'the', 'Portuguese', ',', 'the', 'British', ',', 'the', 'French', ',', 'the', 'Dutch', ',', 'all', 'of', 'them', 'came', 'and', 'looted', 'us', ',', 'took', 'over', 'what', 'was', 'ours', '.'], ['Yet', 'we', 'have', 'not', 'done', 'this', 'to', 'any', 'other', 'nation.We', 'have', 'not', 'conquered', 'anyone', '.'], ['We', 'have', 'not', 'grabbed', 'their', 'land', ',', 'their', 'culture', 'and', 'their', 'history', 'and', 'tried', 'to', 'enforce', 'our', 'way', 'of', 'life', 'on', 'them', '.'], ['Why', '?'], ['Because', 'we', 'respect', 'the', 'freedom', 'of', 'others', '.'], ['That', 'is', 'why', 'my', 'FIRST', 'VISION', 'is', 'that', 'of', 'FREEDOM', '.'], ['I', 'believe', 'that', 'India', 'got', 'its', 'first', 'vision', 'of', 'this', 'in', '1857', ',', 'when', 'we', 'started', 'the', 'war', 'of', 'Independence', '.'], ['It', 'is', 'this', 'freedom', 'that', 'we', 'must', 'protect', 'and', 'nurture', 'and', 'build', 'on', '.'], ['If', 'we', 'are', 'not', 'free', ',', 'no', 'one', 'will', 'respect', 'us.We', 'have', '10', 'percent', 'growth', 'rate', 'in', 'most', 'areas', '.'], ['Our', 'poverty', 'levels', 'are', 'falling', '.'], ['Our', 'achievements', 'are', 'being', 'globally', 'recognised', 'today', '.'], ['Yet', 'we', 'lack', 'the', 'self-confidence', 'to', 'see', 'ourselves', 'as', 'a', 'developed', 'nation', ',', 'self-reliant', 'and', 'self-assured', '.'], ['Isn', '’', 't', 'this', 'incorrect', '?'], ['MY', 'SECOND', 'VISION', 'for', 'India', 'is', 'DEVELOPMENT', '.'], ['For', 'fifty', 'years', 'we', 'have', 'been', 'a', 'developing', 'nation', '.'], ['It', 'is', 'time', 'we', 'see', 'ourselves', 'as', 'a', 'developed', 'nation', '.'], ['We', 'are', 'among', 'top', 'five', 'nations', 'in', 'the', 'world', 'in', 'terms', 'of', 'GDP', '.']]\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcC2E27boXQ",
        "outputId": "7ca3fd2b-4a85-46f4-dc4c-3761f08d6c74"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AQ9sJyUXn2J",
        "outputId": "cbdc672f-05b4-4257-e07d-ae2e8da6d408"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = word_tokenize(sentences[i])\n",
        "  words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = \" \".join(words)"
      ],
      "metadata": {
        "id": "dV1B_q_8bJbL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3pOJg3pcNhI",
        "outputId": "52421fbe-956f-4337-ea2c-fd910e5b662e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation.w conquer anyon .',\n",
              " 'we grab land , cultur histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom other .',\n",
              " 'that first vision freedom .',\n",
              " 'i believ india got first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect us.w 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn ’ incorrect ?',\n",
              " 'my second vision india develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'it time see develop nation .',\n",
              " 'we among top five nation world term gdp .']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk"
      ],
      "metadata": {
        "id": "XmOroou_TIEY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "c2CwUfLDVNaK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer.stem(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "OA7OBNcIVTZe",
        "outputId": "5386f5e4-8ee1-4557-e714-330528046d27"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i have three visions for india. in 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. from alexander onwards the greeks, the turks, the moguls, the portuguese, the british, the french, the dutch, all of them came and looted us, took over what was ours. yet we have not done this to any other nation.we have not conquered anyone. we have not grabbed their land, their culture and their history and tried to enforce our way of life on them. why? because we respect the freedom of others. that is why my first vision is that of freedom. i believe that india got its first vision of this in 1857, when we started the war of independence. it is this freedom that we must protect and nurture and build on. if we are not free, no one will respect us.we have 10 percent growth rate in most areas. our poverty levels are falling. our achievements are being globally recognised today. yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. isn’t this incorrect? my second vision for india is development. for fifty years we have been a developing nation. it is time we see ourselves as a developed nation. we are among top five nations in the world in terms of gdp.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "xn8-QAPgVWVi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer.stem(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "0mDhApV3XdVy",
        "outputId": "8642572e-a503-4804-ed90-233e43da3284"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i have three visions for india. in 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. from alexander onwards the greeks, the turks, the moguls, the portuguese, the british, the french, the dutch, all of them came and looted us, took over what was ours. yet we have not done this to any other nation.we have not conquered anyone. we have not grabbed their land, their culture and their history and tried to enforce our way of life on them. why? because we respect the freedom of others. that is why my first vision is that of freedom. i believe that india got its first vision of this in 1857, when we started the war of independence. it is this freedom that we must protect and nurture and build on. if we are not free, no one will respect us.we have 10 percent growth rate in most areas. our poverty levels are falling. our achievements are being globally recognised today. yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. isn't this incorrect? my second vision for india is development. for fifty years we have been a developing nation. it is time we see ourselves as a developed nation. we are among top five nations in the world in terms of gdp.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTFFoB12XjHE",
        "outputId": "c5e6d7f7-4d13-4d05-e3b2-2212b5682158"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "BC6VM8auYEoj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "_T-jzgm2cowh"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "SzB1Fl8zeyg6"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = word_tokenize(sentences[i])\n",
        "  words = [wordnet_lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = \" \".join(words)"
      ],
      "metadata": {
        "id": "7OHTAIFqe6SA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NtE456KfCaY",
        "outputId": "f4cbaa91-8fd4-4547-a0c9-de203de19041"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india .',\n",
              " '3000 year histori peopl world come invad u , captur land , conquer mind .',\n",
              " 'alexand onward greek , turk , mogul , portugues , british , french , dutch , came loot u , took .',\n",
              " 'yet done nation.w conquer anyon .',\n",
              " 'grab land , cultur histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom .',\n",
              " 'first vision freedom .',\n",
              " 'believ india got first vision 1857 , start war independ .',\n",
              " 'freedom must protect nurtur build .',\n",
              " 'free , one respect us.w 10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'second vision india develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top five nation world term gdp .']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parts of Speech"
      ],
      "metadata": {
        "id": "b_cOE-ODafbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parts of Speech tagging : Part-of-Speech (POS) tagging is a fundamental task in Natural Language Processing (NLP) that involves assigning each word in a sentence a specific part of speech based on its definition and context. The parts of speech include categories such as nouns, verbs, adjectives, adverbs, and more. POS tagging helps in understanding the grammatical structure of a sentence and plays a crucial role in various NLP tasks such as syntactic parsing, text analysis, lemmatization, and machine translation.\n",
        "\n",
        "POS Dictionary (Common POS Tags)\n",
        "Below is a table listing some of the common POS tags used in NLP:\n",
        "\n",
        "POS Tag -\tFull Form\tDescription\n",
        "\n",
        "\n",
        "CC\tCoordinating Conjunction\tConnects words or phrases (e.g., \"and\", \"or\").\n",
        "\n",
        "CD\tCardinal Number\tNumbers (e.g., \"one\", \"two\").\n",
        "\n",
        "DT\tDeterminer\tArticles or words introducing a noun (e.g., \"the\", \"a\").\n",
        "\n",
        "EX\tExistential There\tThe word \"there\" in phrases like \"there is\".\n",
        "\n",
        "FW\tForeign Word\tNon-English words.\n",
        "\n",
        "IN\tPreposition or Subordinating Conjunction\tShows relationships between words (e.g., \"in\", \"on\").\n",
        "\n",
        "JJ\tAdjective\tDescribes nouns (e.g., \"quick\", \"happy\").\n",
        "\n",
        "JJR\tAdjective, Comparative\tComparative adjectives (e.g., \"bigger\", \"faster\").\n",
        "\n",
        "JJS\tAdjective, Superlative\tSuperlative adjectives (e.g., \"biggest\", \"fastest\").\n",
        "\n",
        "MD\tModal\tAuxiliary verbs (e.g., \"can\", \"will\").\n",
        "\n",
        "NN\tNoun, Singular or Mass\tCommon nouns (e.g., \"dog\", \"car\").\n",
        "\n",
        "NNS\tNoun, Plural\tPlural nouns (e.g., \"dogs\", \"cars\").\n",
        "\n",
        "NNP\tProper Noun, Singular\tSingular proper nouns (e.g., \"John\", \"Paris\").\n",
        "\n",
        "NNPS\tProper Noun, Plural\tPlural proper nouns (e.g., \"Americans\").\n",
        "\n",
        "PDT\tPredeterminer\tWords like \"all\" or \"both\" used before determiners.\n",
        "\n",
        "POS\tPossessive Ending\tPossessive suffix (e.g., \"'s\").\n",
        "\n",
        "PRP\tPersonal Pronoun\tPronouns (e.g., \"he\", \"she\", \"they\").\n",
        "\n",
        "PRP$\tPossessive Pronoun\tPossessive pronouns (e.g., \"his\", \"her\", \"our\").\n",
        "\n",
        "RB\tAdverb\tDescribes verbs, adjectives, or other adverbs (e.g., \"quickly\", \"very\").\n",
        "\n",
        "RBR\tAdverb, Comparative\tComparative adverbs (e.g., \"better\").\n",
        "\n",
        "RBS\tAdverb, Superlative\tSuperlative adverbs (e.g., \"best\").\n",
        "\n",
        "RP\tParticle\tSmall words that combine with verbs (e.g., \"give up\", \"put off\").\n",
        "\n",
        "TO\t\"to\"\tThe word \"to\" used as part of infinitives (e.g., \"to go\").\n",
        "\n",
        "UH\tInterjection\tWords expressing emotion (e.g., \"wow\", \"hey\").\n",
        "\n",
        "VB\tVerb, Base Form\tThe base form of a verb (e.g., \"run\").\n",
        "\n",
        "VBD\tVerb, Past Tense\tPast tense verbs (e.g., \"ran\").\n",
        "\n",
        "VBG\tVerb, Gerund or Present Participle\tVerbs ending in \"-ing\" (e.g., \"running\").\n",
        "\n",
        "VBN\tVerb, Past Participle\tPast participle form (e.g., \"eaten\").\n",
        "\n",
        "VBP\tVerb, Non-3rd Person Singular Present\tVerbs like \"run\", \"eat\" for \"I\", \"we\", etc.\n",
        "\n",
        "VBZ\tVerb, 3rd Person Singular Present\tVerbs for 3rd person singular (e.g., \"runs\", \"eats\").\n",
        "\n",
        "WDT\tWh-Determiner\tWords like \"which\", \"that\" in questions.\n",
        "\n",
        "WP\tWh-Pronoun\tQuestion pronouns (e.g., \"who\", \"what\").\n",
        "\n",
        "WP$\tPossessive Wh-Pronoun\tPossessive wh-pronouns (e.g., \"whose\").\n",
        "\n",
        "WRB\tWh-Adverb\tQuestion adverbs (e.g., \"where\", \"when\").\n",
        "\n",
        "Given the sentence:\n",
        "\n",
        "\"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "POS tagging could yield:\n",
        "\n",
        "The → Determiner (DT)\n",
        "\n",
        "quick → Adjective (JJ)\n",
        "\n",
        "brown → Adjective (JJ)\n",
        "\n",
        "fox → Noun (NN)\n",
        "\n",
        "jumps → Verb (VBZ)\n",
        "\n",
        "over → Preposition (IN)\n",
        "\n",
        "the → Determiner (DT)\n",
        "\n",
        "lazy → Adjective (JJ)\n",
        "\n",
        "dog → Noun (NN)"
      ],
      "metadata": {
        "id": "sxzqMk7ObQB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "GnAi8x5MfFiN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "2cI64nPwcSFK",
        "outputId": "447cbf8e-fb0b-47df-c490-30a6ad3d2148"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation.We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "XNuDKHhhceUw"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "PuignKTAcnQe"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqZ-q2Y3csg7",
        "outputId": "0c0a0a2d-9965-4183-e2e8-33dfeffb8179"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture and their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.',\n",
              " 'That is why my FIRST VISION is that of FREEDOM.',\n",
              " 'I believe that India got its first vision of this in 1857, when we started the war of Independence.',\n",
              " 'It is this freedom that we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.We have 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'MY SECOND VISION for India is DEVELOPMENT.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among top five nations in the world in terms of GDP.']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ8SnRlCfvyH",
        "outputId": "af0c2064-808b-4698-e859-d3766e0dcc76"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applying POS Tagging for each word and removing the stop words\n",
        "for i in range(len(sentences)):\n",
        "  words =  nltk.word_tokenize(sentences[i])\n",
        "  words = [word for word in words if word not in set(stopwords.words('english'))]\n",
        "  pos_tag = nltk.pos_tag(words)\n",
        "  print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_136EZIctgU",
        "outputId": "22caeeef-cb8f-457f-b9f8-b605c4a35e6d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('done', 'VBN'), ('nation.We', 'NN'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('?', '.')]\n",
            "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others', 'NNS'), ('.', '.')]\n",
            "[('That', 'DT'), ('FIRST', 'NNP'), ('VISION', 'NNP'), ('FREEDOM', 'NNP'), ('.', '.')]\n",
            "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('war', 'NN'), ('Independence', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
            "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us.We', 'VBZ'), ('10', 'CD'), ('percent', 'NN'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('MY', 'PRP$'), ('SECOND', 'JJ'), ('VISION', 'NNP'), ('India', 'NNP'), ('DEVELOPMENT', 'NNP'), ('.', '.')]\n",
            "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('five', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.pos_tag('Adoni is my HomeTown'.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "994kmiaBdYMb",
        "outputId": "dfea288f-02b5-46a2-fc2a-571f7ffd37ca"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Adoni', 'NNP'), ('is', 'VBZ'), ('my', 'PRP$'), ('HomeTown', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'Adoni is my HomeTown'\n",
        "li = sent.split()\n",
        "li"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aEmOrXKgJlx",
        "outputId": "8b86646e-fd55-480a-f054-33f94b70858c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adoni', 'is', 'my', 'HomeTown']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'Adoni is my HomeTown , coming to the hometown tomorrow'\n",
        "li = []\n",
        "curr_word = \"\"\n",
        "for char in sent:\n",
        "\n",
        "  if char != \" \":\n",
        "    curr_word = curr_word + char\n",
        "\n",
        "  else:\n",
        "    li.append(curr_word)\n",
        "    curr_word = \"\"\n",
        "\n",
        "\n",
        "if curr_word:\n",
        "  li.append(curr_word)\n",
        "\n",
        "\n",
        "print(li)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua5ySJlshRg9",
        "outputId": "891e51fe-54c3-4141-e495-20b79986f139"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Adoni', 'is', 'my', 'HomeTown', ',', 'coming', 'to', 'the', 'hometown', 'tomorrow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " for word in li:\n",
        "  print(nltk.pos_tag([word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWG-WQpOiCW5",
        "outputId": "7b88b037-e81b-403a-dc8e-4e3abcdc629d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Adoni', 'NNP')]\n",
            "[('is', 'VBZ')]\n",
            "[('my', 'PRP$')]\n",
            "[('HomeTown', 'NNP')]\n",
            "[(',', ',')]\n",
            "[('coming', 'VBG')]\n",
            "[('to', 'TO')]\n",
            "[('the', 'DT')]\n",
            "[('hometown', 'NN')]\n",
            "[('tomorrow', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char = \"\"\"The Charminar is a monument located in Hyderabad, Telangana, India. Constructed in 1591, the landmark is a symbol of Hyderabad and officially incorporated in the emblem of Telangana\"\"\""
      ],
      "metadata": {
        "id": "RQw0j93TicMD"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xh4yF7dj2emh",
        "outputId": "53b8272b-ae17-4efd-ecdd-9867c4aafeb7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Charminar is a monument located in Hyderabad, Telangana, India. Constructed in 1591, the landmark is a symbol of Hyderabad and officially incorporated in the emblem of Telangana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "legRL5db2fZP"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char = char.split()"
      ],
      "metadata": {
        "id": "B2RipuoI2isZ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(char))\n",
        "type(char)\n",
        "char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUMaxuTV2q52",
        "outputId": "bd8c445a-5ea2-4a1a-a978-5f4152acfd21"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Charminar',\n",
              " 'is',\n",
              " 'a',\n",
              " 'monument',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Hyderabad,',\n",
              " 'Telangana,',\n",
              " 'India.',\n",
              " 'Constructed',\n",
              " 'in',\n",
              " '1591,',\n",
              " 'the',\n",
              " 'landmark',\n",
              " 'is',\n",
              " 'a',\n",
              " 'symbol',\n",
              " 'of',\n",
              " 'Hyderabad',\n",
              " 'and',\n",
              " 'officially',\n",
              " 'incorporated',\n",
              " 'in',\n",
              " 'the',\n",
              " 'emblem',\n",
              " 'of',\n",
              " 'Telangana']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "XOZLyLf23bM5"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char\n",
        "type(char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHZONzep2rf_",
        "outputId": "d011f564-3dff-4c88-f1b8-8cb62b169374"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "char\n",
        "print(char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMWdUb1F-FXu",
        "outputId": "375d9a45-998d-46c7-ac8a-dc79c7a5ca72"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Charminar', 'is', 'a', 'monument', 'located', 'in', 'Hyderabad,', 'Telangana,', 'India.', 'Constructed', 'in', '1591,', 'the', 'landmark', 'is', 'a', 'symbol', 'of', 'Hyderabad', 'and', 'officially', 'incorporated', 'in', 'the', 'emblem', 'of', 'Telangana']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in char:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWCsjQ-F-82j",
        "outputId": "a6a75c32-68b1-4413-a34a-9d7cc433e3d9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "Charminar\n",
            "is\n",
            "a\n",
            "monument\n",
            "located\n",
            "in\n",
            "Hyderabad,\n",
            "Telangana,\n",
            "India.\n",
            "Constructed\n",
            "in\n",
            "1591,\n",
            "the\n",
            "landmark\n",
            "is\n",
            "a\n",
            "symbol\n",
            "of\n",
            "Hyderabad\n",
            "and\n",
            "officially\n",
            "incorporated\n",
            "in\n",
            "the\n",
            "emblem\n",
            "of\n",
            "Telangana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applying POS Tagging for each word and removing the stop words\n",
        "words = []\n",
        "for i in range(len(char)):\n",
        "  words =  nltk.word_tokenize(char[i])\n",
        "  words = [word for word in words if word not in set(stopwords.words('english'))]\n"
      ],
      "metadata": {
        "id": "pzqNEttl7nh0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q070_Gyg7vIF",
        "outputId": "1a116557-622e-4094-8f92-66ac08bd6033"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Telangana']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22Nh686w-d8U"
      },
      "execution_count": 87,
      "outputs": []
    }
  ]
}